# Introduction:
Object detection in computer vision has become a cornerstone for various real-world applications, ranging from autonomous driving to security systems and image analysis. Deep learning, particularly convolutional neural networks (CNNs), has revolutionized the accuracy and performance of object detection by effectively extracting features from labeled images. The region-based CNN (R-CNN) family stands out as a pivotal model for object identification, incorporating object categorization and region proposal stages. Notable advancements such as Fast R-CNN, Faster R-CNN, and Mask R-CNN have further streamlined processes, enhancing both speed and accuracy. The Single Shot Multibox Detector (SSD) offers another intriguing approach by integrating object classification and region proposal within a single network, eliminating the need for separate proposal creation and significantly reducing inference times. Training on widely used datasets like COCO and Pascal VOC has empowered these models to develop robust object detection and classification skills.
# Abstract
In order to identify, classify, and localize objects in pictures as a unique solution to Computer Vision Real World Domain specific challenges, the generalized object detection framework Faster R-CNN is based on a CNN. Unlike previous object identification algorithms that perceive it as a classification problem, it views object identification as a single regression problem. Due to its efficiency in simultaneously detecting, classifying, and localizing many items from various classes, this complex object detection method has gained popularity. The goal of this project is to build a dataset with various picture formats to implement Faster R-CNN for object detection, classification, and localization. The chosen approach would include detecting, classifying, and localizing persons and objects on the road using a pretrained CNN. This has the potential to be used in active safety systems for driverless cars as well. Additionally, it can be implemented differently to assist with data collection and categorization
## KeyWords:
R-CNN, OBJECT DETECTION AND RECOGNITION
## CNN : With 3 layers:
1.COnvolutionary Layer
2.Dense Layer
3.Max Pooling layer
![image](https://github.com/Ameen-mohammed2003/Object-Detection-Using-Faster-RCNN-CNN/assets/113657529/17c81f01-0019-43fb-8cfc-e182aa884aa6)
## Faster RCNN
![image](https://github.com/Ameen-mohammed2003/Object-Detection-Using-Faster-RCNN-CNN/assets/113657529/4c6b093c-9bc8-4919-9e3b-9531229c1895)
## Fast RCNN
![image](https://github.com/Ameen-mohammed2003/Object-Detection-Using-Faster-RCNN-CNN/assets/113657529/aad6c824-be89-4fee-9bcf-4ae10a9f00af)

# Results:
![image](https://github.com/Ameen-mohammed2003/Object-Detection-Using-Faster-RCNN-CNN/assets/113657529/c8ae518d-5e5a-40a1-8798-a70fd5ecfc57)
# Conclusion:
In conclusion, object detection, which includes locating and recognizing items inside digital photos or videos, is a critical computer vision problem. 
Faster R-CNN, on the other hand, employs a two-stage methodology to get more accuracy. It initially creates a number of area ideas, which are then categorized and improved using a different region classification network. Although it can be computationally more expensive, this technique often performs better in situations where exact object localization is important.
 Faster R-CNN has been a significant contributor to object identification, finding widespread    application in various fields. Its strengths lie in object tracking, aerial picture analysis, and  medical imaging, owing to its enhanced accuracy. Unlike YOLO, which excels in real-time applications, Faster R-CNN's impact is more pronounced in scenarios where precision is paramount.
 Anticipated advancements in Faster R-CNN and related methodologies are likely to center on refining occlusion handling, bolstering adaptability across diverse scenarios, and leveraging contextual cues. These developments aim to further enhance its capabilities in perceiving complex environments. Ultimately, these strides will aid in augmenting how robots interpret and engage with the visual world.

# Team Members:
1.AP21110010948 - Ameen Mohammed

2.AP21110010915 - Abhinav Uppaluri

3.AP21110010931 - Ganesh sesha sai Akhil . Koutarapu

4.AP21110010962 - Sai Amruth . D

5.AP21110010970 - Venkata Arun Kumar . Perla
